---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r, code = readLines("common.R"), cache = FALSE, include=FALSE}
```

```{r links, child="links.md"}
```

```{r, include=FALSE}
module_number_prefix <- "15"
module_number <- as.numeric(module_number_prefix)
module_name <- "r-dist-fit"
here::i_am(str_c("book/", module_number_prefix, "_", module_name, ".Rmd"))
project_name_prefix <- str_c("TM", module_number)
```

# (PART) Extra R {-} 

# Fitting probability distributions {#mod-r-dist-fit}

Fitting distributions to data is a very common task in statistics and consists in choosing a probability distribution modelling the random variable, as well as finding parameter estimates for that distribution. This requires judgment and expertise and generally needs an iterative process of distribution choice, parameter estimation, and quality of fit assessment. In this module we will limit us to univariate data, i.e. univariate distributions such as normal, uniform, Poisson and exponential distributions. We will use  the [`fitdistrplus`](https://lbbe.univ-lyon1.fr/fr/fitdistrplus) package for fitting distributions.

`r link_rcloud_text(module_number)`

<!-- #### Learning path diagram {-} -->

```{r, echo=FALSE, out.width="100%", fig.asp=NA, include=FALSE, fig.height=3, eval=FALSE}
g <- create_learning_path(
   url = "https://docs.google.com/spreadsheets/d/1bBe42LHK-bE7CsU9eNBzi_7VNjbmv-Ybr7285pE61jM/edit?usp=sharing", 
   sheet = "r-dist-fit", 
   x_legend = 0, y_legend = 0.55)
render_graph(g)
learning_path_text_r()
```

## Learning outcomes {#lo-r-dist-fit}

By the end of this module, you are expected to:

* Have knowledge about different univariate distributions. 
* Identify continuous and discrete data.
* Fit different distributions to data. 

The learning outcomes relate to the [overall learning goals](#lg-course) number 7 and 11-14 of the course.

<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->


## Introduction {#sec-r-dist-fit-intro}

Fitting distributions to data is a very common task in statistics and consists in choosing a probability distribution modelling the random variable, as well as finding parameter estimates for that distribution. This requires judgment and expertise and generally needs an iterative process of distribution choice, parameter estimation, and quality of fit assessment. In this module we will limit us to univariate data, i.e. univariate distributions such as a normal, uniform, Poisson and exponential distribution.

Consider a dataset $x = (x_1, \ldots, x_n)$ with $n$ observations which is assumed to be _sample_ observations of a random variable $X$. Our goal is to find a distribution that fit the data well and estimate the parameters of the distribution. For instance if the distribution is a normal distribution then the mean and variance should be estimated. 

Finding the distribution is an iterative process by considering distribution choice, parameter estimation, and quality of fit assessment repeatedly until you are satisfied. The first step is to decide if you data is should fit a discrete or continuous distribution. That is, should the random variable always take discrete values or could/are continuous values possible/okay? Note even though your sample only contain discrete values it is not necessarily samples from a discrete distribution. Consider for instance the dataset `groundbeef` which contains values of serving sizes in grams of ground beef patties consumed by children under 5 years old:

```{r}
library(fitdistrplus)
library(tidyverse)
data("groundbeef")   # activate the dataset
dat <- as_tibble(groundbeef)
str(dat)
```

Serving size in grams is only given using integers; however it is obvious that a continuous distribution should be fitted. We select the `serving` column so `dat` becomes a vector with the observations.

```{r}
dat <- dat$serving
```


## Fitting distributions to continuous data {#sec-r-dist-fit-cont}

Before fitting one or more distributions to a data set, it is generally necessary to choose good
candidates among a predefined set of distributions. This choice may be guided by the knowledge of
stochastic processes governing the modeled variable, or, in the absence of knowledge regarding the
underlying process, by the observation of its empirical distribution. We will use the
[`fitdistrplus`](https://lbbe.univ-lyon1.fr/fr/fitdistrplus) package for fitting distributions. Let
us continue with the `groundbeef` dataset.

First of all, it is common to start with plots of the empirical distribution function and the
histogram (or density plot), which can be obtained with the `plotdist` function which provides two
plots where the left-hand plot is by default the histogram on a density scale and the
right-hand plot the empirical cumulative distribution function (CDF).

```{r, fig.height=6, fig.cap="Empirical density and distribution."}
plotdist(dat, histo = TRUE, demp = TRUE)
```

The empirical plots of the density and the CDF may give you a hit about the distribution of $X$. But
often additional descriptive statistics may help to choose candidates to describe a distribution
among a set of parametric distributions. Especially the skewness and kurtosis, linked to the third
and fourth moments, are useful for this purpose. A non-zero skewness reveals a lack of symmetry of
the empirical distribution, while the kurtosis value quantifies the weight of tails in comparison to
the normal distribution for which the kurtosis equals 3.

The skewness and kurtosis and their corresponding estimate are given by
  \begin{equation}
    \label{skewness}
    sk(X) = \frac{E[(X-E(X))^3]}{Var(X)^{\frac{3}{2}}}~,~
    \widehat{sk}=\frac{\sqrt{n(n-1)}}{n-2}\times\frac{m_{3}}{m_{2}^{\frac{3}{2}}},
  \end{equation}

  \begin{equation}
    \label{kurtosis}
    kr(X) = \frac{E[(X-E(X))^4]}{Var(X)^{2}}~,~
    \widehat{kr}=\frac{n-1}{(n-2)(n-3)}((n+1) \times \frac{m_{4}}{m_{2}^{2}}-3(n-1)) + 3,
  \end{equation}
where $m_{2}$, $m_{3}$, $m_{4}$ denote empirical moments defined by
$m_{k}=\frac{1}{n}\sum_{i=1}^n(x_{i}-\overline{x})^{k}$, with mean value $\overline{x}$.

The `descdist` function provides classical descriptive statistics (minimum, maximum, median, mean,
standard deviation), skewness and kurtosis. A skewness-kurtosis plot such as the one proposed by
@Cullen99 is provided by the `descdist` function for the empirical distribution On this plot, values
for common distributions are displayed in order to help the choice of distributions to fit to
data. For some distributions (normal, uniform, logistic, exponential), there is only one possible
value for the skewness and the kurtosis. Thus, the distribution is represented by a single point
on the plot. For other distributions, areas of possible values are represented, consisting of
lines (as for gamma and lognormal distributions), or areas (as for beta distribution).
Nevertheless, the user needs to know that skewness and kurtosis, like all higher moments, have a
very high variance. Hence the skewness-kurtosis plot should then be regarded as indicative only
and the properties of the random variable should be considered, notably its expected value and its
range, as a complement to the use of the `plotdist` and `descdist` functions.

Below is a call to the `descdist` function to describe the distribution of the serving size from the
`groundbeef` data set and to draw the corresponding skewness-kurtosis plot. Looking at the results
notice that the observed skewness is positive and the kurtosis is not far from 3. Hence the fit of
three common right-skewed distributions could be considered: Weibull, gamma and lognormal
distributions.

```{r, fig.cap="Skewness-kurtosis plot for a continuous variable (`groundbeef`)."}
descdist(dat)
```

Once one or more parametric distributions have been selected they are fitted to the data set using maximum likelihood. This is done using the `fitdist` function:

```{r}
fitW <- fitdist(dat, distr = "weibull")
fitG <- fitdist(dat, distr = "gamma")
fitL <- fitdist(dat, distr = "lnorm")
```

The function returns a list with information about the fit such as the parameter estimates, the loglikelihood, the Akaike and Bayesian information criteria (the so-called AIC and BIC). An overview can be seen using the `summary` function:

```{r}
summary(fitW)
summary(fitG)
summary(fitL)
```

The fit to the data can be plotted using four classical goodness-of-fit plots [@Cullen99]:

- a density plot representing the density function of the fitted distribution along with the
histogram of the empirical distribution,
- a CDF plot of both the empirical distribution and the fitted distribution,
- a Q-Q plot representing the empirical quantiles (y-axis) against the theoretical quantiles
(x-axis),
- a P-P plot representing the empirical distribution function evaluated at each data point (y-axis)
against the fitted distribution function (x-axis).

```{r, fig.asp=1, out.width="100%", fig.cap="Four Goodness-of-fit plots for various distributions fitted to the serving size data."}
par(mfrow = c(2, 2))  # 4 plots in one figure
pLegend <- c("Weibull", "lognormal", "gamma")
lst <- list(fitW, fitL, fitG)
denscomp(lst, legendtext = pLegend)
qqcomp(lst, legendtext = pLegend)
cdfcomp(lst, legendtext = pLegend)
ppcomp(lst, legendtext = pLegend)
```

The density plot and the CDF plot may be considered as the basic classical goodness-of-fit plots.
The two other plots are complementary and can be very informative in some cases. The Q-Q plot
emphasizes the lack-of-fit at the distribution tails while the P-P plot emphasizes the lack-of-fit
at the distribution center. In the present example, none of the three fitted distributions correctly
describes the center of the distribution, but the Weibull and gamma distributions could be preferred
for their better description of the right tail of the empirical distribution. Moreover, these
distributions also have the lowest AIC values.

Other methods can be used to fit distributions to the data instead of maximum likelihood. For details see @DelignetteMuller15.

### Example: breakdown times 

We consider a dataset that contains the recorded breakdown times of a machine part in days.   
```{r}
library(tfa)  # update to latest version using remotes::install_github("bss-osca/tfa-package", upgrade = FALSE) 
dat <- breakdown
plotdist(dat, histo = TRUE, demp = TRUE)
```

Since breakdown times may be seen as a continuous variable we try to fit a continuous variable. First we try to find distribution candidates:

```{r}
descdist(dat)
```

The observation is close to the normal and lognormal values and we try to fit these two distributions:

```{r, fig.asp=1, out.width="100%", fig.cap="Four Goodness-of-fit plots for various distributions fitted to the breakdown data."}
fitN <- fitdist(dat, distr = "norm")
fitL <- fitdist(dat, distr = "lnorm")
summary(fitN)
summary(fitL)
par(mfrow = c(2, 2))  # 4 plots in one figure
pLegend <- c("normal", "lognormal")
lst <- list(fitN, fitL)
denscomp(lst, legendtext = pLegend)
qqcomp(lst, legendtext = pLegend)
cdfcomp(lst, legendtext = pLegend)
ppcomp(lst, legendtext = pLegend)
```

Note it seems that the normal distribution has a better fit (AIC smallest). Also intuitively it makes sense that breakdown times are normal distributed around a mean value (however the probability of negative values should be low).  







## Fitting distributions to discrete data {#sec-r-dist-fit-discrete}

You may also need to fit discrete distributions such as:

- The Poisson distribution which is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event. The Poisson distribution can be applied to systems with a large number of possible events, each of which is rare. How many such events will occur during a fixed time interval? Under the right circumstances, this is a random number following a Poisson distribution.

- The binomial distribution which is a probability distribution that is used to model the number of successes in a sequence of $n$ independent experiments with probability $p$ for success. The Bernoulli distribution is a special case of the Binomial distribution where $n=1$ (one experiment).

- The negative binomial distribution which is a discrete probability distribution that models the number of failures in a sequence of independent and identically distributed Bernoulli trials before a specified number of successes occurs.

- The geometric distribution which is a special case of the negative binomial distribution where the specified number of successes are one.

### Example: sales of lottery tickets

Consider data of the number of houses that must be visited before selling 20 lottery tickets (assuming only one is sold at a time). 

```{r}
dat <- lottery
plotdist(dat, discrete = TRUE)
```

Since the number of houses that must be visited (trials) is unknown, a negative binomial distribution seems as a good choice (with 20 successes). The fit of a discrete distribution to discrete data requires the same procedure as for continuous data.

```{r, fig.asp=0.6, out.width="100%", fig.cap="Four Goodness-of-fit plots for various distributions fitted to the breakdown data."}
fit <- fitdist(dat, distr = "nbinom", discrete = TRUE, fix.arg = list(size = 20))
summary(fit)
par(mfrow = c(1, 2))  # 2 plots in one figure
pLegend <- c("negative binomial")
lst <- list(fit)
denscomp(lst, legendtext = pLegend)
cdfcomp(lst, legendtext = pLegend)
```

Note only the density and CDF is plotted for discrete data. Moreover, this is an example where some of the parameters of the negative binomial distribution are known in advance. The negative binomial distribution has two parameters (see `?pnbinom`): the specified number of successes to occur (`size` argument) and the probability of success in each trial (`prob` argument). Given the information about the dataset, `size` equals 20 and we want to to try finding the probability of success in each trial. You fix a parameter by using the `fix.arg` argument in `fitdist`. Note that the summary only outputs the mean estimate `mu` but `prob = size/(size+mu)`:

```{r}
prob <- 20/(20 + fit$estimate)
names(prob) <- NULL
prob
```

That is the probability of success when visiting each house is approx. `r round(prob*100)`% and e.g. the number of houses that must be visited before all lottery tickets are sold with 95% probability is:

```{r}
qnbinom(0.95, size = 20, prob)
```


## The Poisson process {#sec-dist-fit-pois}

A Poisson process models a series of discrete events. The average time between events is known, but the exact timing of events is random: the waiting time between events are exponential distributed. Since the exponential distribution is memoryless, the arrival time of an event is independent of what happend in the past (e.g. the arrival time of the last event). A Poisson process is often used in queueing theory to model random events, such as the arrival of customers at a store or phone calls arriving at a call center.

More formally a Poisson process is defined as process counting the total number of events $N(t)$ at time $t$ given the average arrival rate $\lambda$. The process satisfy that:

- $N(0)=0$;
- The waiting time between events is exponential distributed with rate $\lambda$;
- The number of arrivals in any interval of length $\tau$ is Poisson distributed with parameter $\tau\lambda$

The arrivals in a Poisson process with on average 4 arrivals per time unit ($\lambda=4$) may look like:

```{r poisson_process, echo=FALSE}
arrivals<-10 # we sample 10 arrivals
f<-rexp(arrivals,rate=4)   # sample 10 interarrival times with a rate of 4 arrivals/time unit
x<-c(0,stats::filter(f,1, method="recursive"))   # arrival time of the 10 arrivals S_n
xP<-x[2:(arrivals+1)]
dat<-data.frame(S=c(x,xP),N=c(0:arrivals,0:(arrivals-1)))
dat<-dat[order(dat$S,dat$N),]
p<-rep(c(1,0),times=length(dat$S))
dat$p<-p[1:length(dat$S)]
ggplot(data=dat, aes(x=S, y=N, color=p) ) +
   geom_line() + geom_point(aes(shape=factor(p))) +
   scale_shape_manual(values=c(1,19)) + theme(legend.position="none") +
   xlab("time") + ylab("arrivals")
```

Let us consider the data set `demand_goods` containing demand for two different goods/products:

```{r}
library(tfa)  # update to latest version using remotes::install_github("bss-osca/tfa-package", upgrade = FALSE) 
demand_goods
```

The arrival of demands and demand size may follow a compound Poisson process which differ from a Poisson process by allowing more than a demand of one given an arrival. That is, the inter arrival times should still follow an exponential distribution. Let us find the inter arrival times between each order and try to fit it: 

```{r}
library(tidyverse)
datDf <- demand_goods |> 
  group_by(product) |> 
  mutate(between = as.numeric(date - lag(date), units="days")) |> 
  print()

dat <- datDf |> 
  filter(product == 1, !is.na(between)) |> 
  pull(between)
plotdist(dat, histo = TRUE, demp = TRUE)
descdist(dat)
```

The observation is close to the exponential and lognormal values. However, a lognormal distribution cannot equal zero values:

```{r, fig.asp=1, out.width="100%", fig.cap="Four Goodness-of-fit plots for various distributions fitted to the breakdown data."}
fitE1 <- fitdist(dat, distr = "exp")
summary(fitE1)
par(mfrow = c(2, 2))  # 4 plots in one figure
pLegend <- c("exp")
lst <- list(fitE1)
denscomp(lst, legendtext = pLegend)
qqcomp(lst, legendtext = pLegend)
cdfcomp(lst, legendtext = pLegend)
ppcomp(lst, legendtext = pLegend)
```

The fit seems okay. Note that the horizontal and vertical points in the Q-Q and P-P plot indicate that another scale on measuring the inter arrival times would have been better (such as hours or minutes). Using days may round the numbers to much.

Let us do the analysis for product 2:

```{r}
dat <- datDf |> 
  filter(product == 2, !is.na(between)) |> 
  pull(between)
plotdist(dat, histo = TRUE, demp = TRUE)
descdist(dat)
```

The observation is close to the exponential values:

```{r, fig.asp=1, out.width="100%", fig.cap="Four Goodness-of-fit plots for various distributions fitted to the breakdown data."}
fitE2 <- fitdist(dat, distr = "exp")
summary(fitE2)
par(mfrow = c(2, 2))  # 4 plots in one figure
pLegend <- c("exp")
lst <- list(fitE2)
denscomp(lst, legendtext = pLegend)
qqcomp(lst, legendtext = pLegend)
cdfcomp(lst, legendtext = pLegend)
ppcomp(lst, legendtext = pLegend)
```

Again the fit seems okay. 

To summarize the demand rate for product 1 is estimated to be `r round(fitE1$estimate,2)` per day and for product 2 to be `r round(fitE2$estimate,2)` per day.

Given a demand arrive the question is how much is the demand size? That is, we have to fit a distribution to the demand. Let us do the analysis for product 2:

```{r}
dat <- datDf |> 
  filter(product == 2) |> 
  pull(demand)
plotdist(dat, histo = TRUE, demp = TRUE, discrete = TRUE)
```

Let us try to fit the negative binomial, Poisson and geometric distribution:

```{r, fig.asp=1, out.width="100%", fig.cap="Four Goodness-of-fit plots for various distributions fitted to the breakdown data."}
fitN2 <- fitdist(dat, distr = "nbinom")
fitP2 <- fitdist(dat, distr = "pois")
fitG2 <- fitdist(dat, distr = "geom")
summary(fitN2)
summary(fitP2)
summary(fitG2)
par(mfrow = c(1, 2)) 
pLegend <- c("neg binomial", "Poisson", "geometric")
lst <- list(fitN2, fitP2, fitG2)
denscomp(lst, legendtext = pLegend)
cdfcomp(lst, legendtext = pLegend)
```

The negative binomial seems to give the best fit and may be a good candidate.


## Recap {#rc-r-dist-fit}

  * Fitting distributions to data is a very common task in statistics and consists in choosing a probability distribution modelling the random variable, as well as finding parameter estimates for that distribution. 
  * Fitting a univariate distribution requires judgment and expertise and generally needs an iterative process of distribution choice, parameter estimation, and quality of fit assessment. Steps may be:
    - Examine data a decide on discrete vs continuous distribution.
    - Find a set of candidate distributions.
    - Fit the distributions using statistical methods and consider various plots.
    - Decide on a distribution
  * The AIC value may give you an indication about the best model fit. 
  * Other methods can be used to fit distributions to the data instead of maximum likelihood. For details see @DelignetteMuller15.
  * This module have only considered uncensored data. See @DelignetteMuller15 on how to handle censored data. 
    
```{r, echo=FALSE}
link_slide_file_text(module_number_prefix, module_name)
```

## Exercises {#ex-r-dist-fit}

`r exercises_r_text(project_name_prefix)`

### Exercise - Call center

Consider data from a call center: the Los Angeles 311 Call Center in 2014 from 8-17:

```{r}
library(tidyverse)
library(tfa)
library(skimr)
skim(calls)
```

We first transform the dataset a bit:

```{r}
library(lubridate)
calls <- calls |>
  group_by(date) |> 
  arrange(date, time) |> 
  mutate(
    arrival = row_number(),
    wday = wday(date, label = TRUE),
    hour = hour(time),
    between = time - lag(time)
  ) 
```

```{r, solution=TRUE, text="`arrival` = number of arrivals on the given day, `wday` = weekday, `hour` = hour considered, `between` = time between calls in secs (inter arrival times)."}
```
  (1) Explain the new columns.

The number of calls may follow a Poisson process with a fixed rate of calls per hour during the day. Let us try to plot the hourly rates:

```{r}
calls |> 
  count(date, hour, wday) |> 
  group_by(hour, wday) |> 
  summarize(rate = mean(n)) |> 
  ggplot(aes(x = hour, y = rate)) + 
  geom_line() + 
  facet_wrap(~wday) + 
  ylab("calls/hour")
```

```{r, solution=TRUE, text="As can be seen the rate is not constaint over the day (we don't have a homogeneous Possion process). Moreover, in weekends the rate is much lower."}
```
  (2) Is the rate constant during a day and is the rate the same for different weekdays (explain)?
  
Since the rate is not fixed we may have a non-homogeneous Poisson process where the rate change during the day. Hence let us try to consider an specific hour and test if the rate here can be considered as fixed (i.e. we have a Poisson process with a fixed rate when considering a specific hour) 

```{r, solution=TRUE, text="For a Poisson process the inter arrival times should follow an exponential distribution. It seems to be a good fit here."}
library(fitdistrplus)
dat <- calls |> 
  filter(
    wday == "Tue",
    hour == 10,
    !is.na(between)) |> 
  pull(between) |> 
  as.numeric() 
descdist(dat)
fit1 <- fitdist(dat, distr = "exp")
fit2 <- fitdist(dat, distr = "lnorm")
fit3 <- fitdist(dat, distr = "gamma")
summary(fit1)
summary(fit2)
summary(fit3)
par(mfrow = c(2, 2))  # 4 plots in one figure
pLegend <- c("exp", "lnorm", "gamma")
lst <- list(fit1, fit2, fit3)
denscomp(lst, legendtext = pLegend)
qqcomp(lst, legendtext = pLegend)
cdfcomp(lst, legendtext = pLegend)
ppcomp(lst, legendtext = pLegend)
```
```{r, hint=TRUE, eval=FALSE}
library(fitdistrplus)
dat <- calls |> 
  filter(
    wday == ___,
    hour == ___
    !is.na(between)) |> 
  pull(between) |> 
  as.numeric() |> 
  print()
descdist(dat)
___ # try to fit the exp, lnorm and gamma
```  
  (3) Consider Tuesdays from 10-11 and fit the inter arrival times (`between`). If the data follow a Poisson process then the distribution should be?



```{r, solution=TRUE}
library(fitdistrplus)
dat <- calls |> 
  filter(
    wday == "Tue",
    hour == 15,
    !is.na(between)) |> 
  pull(between) |> 
  as.numeric() 
descdist(dat)
fit12 <- fitdist(dat, distr = "exp")
fit22 <- fitdist(dat, distr = "lnorm")
fit32 <- fitdist(dat, distr = "gamma")
summary(fit12)
summary(fit22)
summary(fit32)
par(mfrow = c(2, 2))  # 4 plots in one figure
pLegend <- c("exp", "lnorm", "gamma")
lst <- list(fit12, fit22, fit32)
denscomp(lst, legendtext = pLegend)
qqcomp(lst, legendtext = pLegend)
cdfcomp(lst, legendtext = pLegend)
ppcomp(lst, legendtext = pLegend)
```
  (4) Consider Tuesdays from 15-16 and fit the inter arrival times (`between`). 
  
```{r, solution=TRUE, text="The rate is higher (40 calls more per hour) in period 10-11."}
fit1$estimate * 60 * 60 # 10-11
fit12$estimate * 60 * 60 # 15-16
```
```{r, hint=TRUE, eval=FALSE, text="You have to look at the estimates of the rate."}
```  
  (5) What is the estimated arrival rate per hour Tuesday 10-11 and 15-16? Is the arrival rate the same?
  





















